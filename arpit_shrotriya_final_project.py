# -*- coding: utf-8 -*-
"""Arpit_Shrotriya_Final_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19e_lzJCRsUJsh6WaYB_qLLFPcHBf4qeh
"""

#!pip install pandas numpy scikit-learn tensorflow matplotlib seaborn
#!pip install imblearn

import os
import sys
import subprocess

# List of required libraries
required_libraries = [
    "pandas", "numpy", "scikit-learn", "tensorflow", "matplotlib", "seaborn"
]

# Function to install missing libraries
def install_missing_libraries():
    for library in required_libraries:
        try:
            _import_(library)
        except ImportError:
            print(f"Installing missing library: {library}")
            subprocess.check_call([sys.executable, "-m", "pip", "install", library])

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve, brier_score_loss, auc
from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.utils import resample
from sklearn.feature_selection import SelectKBest, f_classif

df = pd.read_csv('mushroom_cleaned.csv')

if 'name' in df.columns:
    df = df.drop(columns=['name'])

df = df.fillna(df.median())

df = pd.get_dummies(df)

df

df.describe()

df.dtypes

X = df.drop(columns=['class'])
y = df['class']

# Determining Correlation between different attributes
fig, axis = plt.subplots(figsize=(5, 5))
correlation_matrix = X.corr()
sns.heatmap(correlation_matrix, annot=True, linewidths=5, fmt='.2f', ax=axis)
plt.show()

top_features = df.corr().abs().sum().sort_values(ascending=False).index[:5]
sns.pairplot(df[top_features], height=2.5)
plt.suptitle("Pairplot of Top Features", y=1.02)
plt.show()

X.hist(bins=15, figsize=(15, 10), layout=(3, 4))
plt.suptitle("Feature Distributions")
plt.show()

# Checking for Data Imbalance
positive_op, negative_op = y.value_counts()
total_samples = y.count()

print("------ Label Imbalance Check ------")
print(f'Number of positive outcomes: {positive_op}, Percentage of positive outcomes: {(positive_op/total_samples)*100:.2f}')
print(f'Number of negative outcomes: {negative_op}, Percentage of negative outcomes: {(negative_op/total_samples)*100:.2f}')

majority = df[df['class'] == 0]
minority = df[df['class'] == 1]
minority_upsampled  = resample(minority, replace=True, n_samples=len(majority), random_state=42)
df_balanced = pd.concat([majority, minority_upsampled])

X = df_balanced.drop(columns=['class'])
y = df_balanced['class']

# Feature Selection
selector = SelectKBest(score_func=f_classif, k=5)
X_selected = selector.fit_transform(X, y)

# Feature Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_selected)

X_reshaped = np.expand_dims(X_scaled, axis=1)

kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

results_rf, results_nb, results_lstm = [], [], []

def calculate_metrics(tp, fp, fn, tn):
    TPR = tp / (tp + fn) if (tp + fn) > 0 else 0
    TNR = tn / (tn + fp) if (tn + fp) > 0 else 0
    FPR = fp / (fp + tn) if (fp + tn) > 0 else 0
    FNR = fn / (fn + tp) if (fn + tp) > 0 else 0

    Precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    Recall = TPR
    F1 = 2 * (Precision * Recall) / (Precision + Recall) if (Precision + Recall) > 0 else 0
    Accuracy = (tp + tn) / (tp + tn + fp + fn)
    Error_rate = 1 - Accuracy

    BACC = (TPR + TNR) / 2
    TSS = TPR - FNR
    HSS = 2 * (tp * tn - fp * fn) / ((tp + fn) * (fn + tn) + (tp + fp) * (fp + tn)) if ((tp + fn) * (fn + tn) + (tp + fp) * (fp + tn)) > 0 else 0

    BS = ((fp + fn) / (tp + tn + fp + fn)) ** 2

    return {
        "TPR": TPR, "TNR": TNR, "FPR": FPR, "FNR": FNR,
        "Recall": Recall, "Precision": Precision, "F1": F1,
        "Accuracy": Accuracy, "Error_rate": Error_rate, "BACC": BACC,
        "TSS": TSS, "HSS": HSS, "BS": BS
    }

for train_index, test_index in kf.split(X_scaled, y):
    # Get initial train-test split
    X_train_full, X_test = X_scaled[train_index], X_scaled[test_index]
    y_train_full, y_test = y.iloc[train_index], y.iloc[test_index]

    # Further split training data into 60% train and 40% validation
    X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.4, random_state=42, stratify=y_train_full)
    X_train_lstm, X_val_lstm = np.expand_dims(X_train, axis=1), np.expand_dims(X_val, axis=1)

    # Random Forest with enhanced regularization
    rf_model = RandomForestClassifier(n_estimators=50, max_depth=8, min_samples_split=15, min_samples_leaf=7, random_state=42, verbose=1)
    rf_model.fit(X_train, y_train)

    # Evaluate on validation set
    y_pred_val_rf = rf_model.predict(X_val)
    tp_val_rf, fp_val_rf, fn_val_rf, tn_val_rf = confusion_matrix(y_val, y_pred_val_rf).ravel()
    print("Random Forest Validation Metrics:", calculate_metrics(tp_val_rf, fp_val_rf, fn_val_rf, tn_val_rf))

    # Evaluate on test set
    y_pred_rf = rf_model.predict(X_test)
    tp_rf, fp_rf, fn_rf, tn_rf = confusion_matrix(y_test, y_pred_rf).ravel()
    results_rf.append(calculate_metrics(tp_rf, fp_rf, fn_rf, tn_rf))

for train_index, test_index in kf.split(X_scaled, y):
    # Split data into train and test for the current fold
    X_train_full, X_test = X_scaled[train_index], X_scaled[test_index]
    y_train_full, y_test = y.iloc[train_index], y.iloc[test_index]

    # Further split training data into 60% train and 40% validation
    X_train, X_val, y_train, y_val = train_test_split(
        X_train_full, y_train_full, test_size=0.4, random_state=None, stratify=y_train_full
    )

    # Naive Bayes Model
    nb_model = GaussianNB(var_smoothing=1e-8)
    nb_model.fit(X_train, y_train)

    # Evaluate on validation set
    y_pred_val_nb = nb_model.predict(X_val)
    #y_prob_val_nb = nb_model.predict_proba(X_val)[:, 1]  # For probabilistic metrics
    tp_val_nb, fp_val_nb, fn_val_nb, tn_val_nb = confusion_matrix(y_val, y_pred_val_nb).ravel()
    print("Naive Bayes Validation Metrics:", calculate_metrics(tp_val_nb, fp_val_nb, fn_val_nb, tn_val_nb))

    # Evaluate on test set
    y_pred_test_nb = nb_model.predict(X_test)
    #y_prob_test_nb = nb_model.predict_proba(X_test)[:, 1]
    tp_val_nb, fp_val_nb, fn_val_nb, tn_val_nb = confusion_matrix(y_val, y_pred_val_nb).ravel()
    results_nb.append(calculate_metrics(tp_val_nb, fp_val_nb, fn_val_nb, tn_val_nb))
    #print("Naive Bayes Validation Metrics:", )

for train_index, test_index in kf.split(X_scaled, y):
    X_train_full, X_test = X_scaled[train_index], X_scaled[test_index]
    y_train_full, y_test = y.iloc[train_index], y.iloc[test_index]

    # Further split training data into 60% train and 40% validation
    X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.4, random_state=42, stratify=y_train_full)
    X_train_lstm, X_val_lstm = np.expand_dims(X_train, axis=1), np.expand_dims(X_val, axis=1)


    # LSTM with early stopping and enhanced regularization
    lstm_model = Sequential()
    lstm_model.add(LSTM(32, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]), activation='relu', kernel_regularizer='l2'))
    lstm_model.add(Dropout(0.4))
    lstm_model.add(Dense(1, activation='sigmoid', kernel_regularizer='l2'))
    lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
    lstm_model.fit(X_train_lstm, y_train, validation_data=(X_val_lstm, y_val), epochs=15, batch_size=64, callbacks=[early_stopping], verbose=1)

    # Evaluate on test set
    y_pred_lstm = (lstm_model.predict(np.expand_dims(X_test, axis=1)) > 0.5).astype("int32")
    tp_lstm, fp_lstm, fn_lstm, tn_lstm = confusion_matrix(y_test, y_pred_lstm).ravel()
    results_lstm.append(calculate_metrics(tp_lstm, fp_lstm, fn_lstm, tn_lstm))

metrics_rf_df = pd.DataFrame(results_rf)
metrics_nb_df = pd.DataFrame(results_nb)
metrics_lstm_df = pd.DataFrame(results_lstm)

# Compute Average Metrics
average_rf = metrics_rf_df.mean()
average_nb = metrics_nb_df.mean()
average_lstm = metrics_lstm_df.mean()

# Display Results
print("Random Forest Results (Per Fold):\n", metrics_rf_df)
print("Random Forest Average Metrics:\n", average_rf)

print("Naive Bayes Results (Per Fold):\n", metrics_nb_df)
print("Naive Bayes Average Metrics:\n", average_nb)

print("LSTM Results (Per Fold):\n", metrics_lstm_df)
print("LSTM Average Metrics:\n", average_lstm)

# Obtain predicted probabilities
y_score_rf = rf_model.predict_proba(X_scaled)[:, 1]
# Compute ROC curve and ROC area
fpr_rf, tpr_rf, _ = roc_curve(y, y_score_rf)
roc_auc_rf = auc(fpr_rf, tpr_rf)

plt.figure(figsize=(8, 6))
plt.plot(fpr_rf, tpr_rf, color="darkorange", label="Random Forest ROC curve‚ê£(area = {:.2f})".format(roc_auc_rf))
plt.plot([0, 1], [0, 1], color="navy", linestyle="--")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Random Forest ROC Curve")
plt.legend(loc="lower right")
plt.show()

# Obtain predicted probabilities
y_score_nb = nb_model.predict_proba(X_scaled)[:, 1]
# Compute ROC curve and ROC area
fpr_nb, tpr_nb, _ = roc_curve(y, y_score_nb)
roc_auc_nb = auc(fpr_nb, tpr_nb)

# Plot SVM ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr_nb, tpr_nb, color="darkorange", label="AUC (area = {:.2f})".format(roc_auc_nb))
plt.plot([0, 1], [0, 1], color="navy", linestyle="--")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Naive Bayes ROC Curve")
plt.legend(loc="lower right")
plt.show()

# Predict probabilities for the test set
predict_lstm = lstm_model.predict(X_reshaped)
# Compute ROC curve and ROC area
fpr_lstm, tpr_lstm, _ = roc_curve(y, predict_lstm)
roc_auc_lstm = auc(fpr_lstm, tpr_lstm)
# Plot LSTM ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr_lstm, tpr_lstm, color="darkorange", label="AUC (area = {:.2f})".format(roc_auc_lstm))
plt.plot([0, 1], [0, 1], color="navy", linestyle="--")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("LSTM ROC Curve")
plt.legend(loc="lower right")
plt.show()

